[pytest]
# ===== pytest Configuration File =====
#
# This configuration file defines pytest behavior, test discovery patterns,
# coverage settings, and reporting options.
#
# @see https://docs.pytest.org/en/stable/reference/customize.html

# ===== Test Discovery =====

# Patterns for test file discovery
python_files = test_*.py *_test.py tests.py

# Patterns for test function discovery
python_functions = test_*

# Patterns for test class discovery
python_classes = Test*

# Directories to search for tests (relative to project root)
testpaths = tests

# Minimum Python version required
minversion = 7.0

# ===== Test Execution =====

# Add current directory to PYTHONPATH
pythonpath = . src

# Run tests in parallel (requires pytest-xdist plugin)
# Uncomment to enable: addopts = -n auto
addopts =
    # Verbose output
    -v
    # Show extra test summary info
    -ra
    # Show local variables in tracebacks
    -l
    # Strict markers (fail on unknown markers)
    --strict-markers
    # Strict config (fail on unknown config keys)
    --strict-config
    # Enable coverage reporting
    --cov=src
    --cov=app
    # Coverage report format
    --cov-report=html
    --cov-report=term-missing
    --cov-report=xml
    # Fail if coverage is below threshold
    --cov-fail-under=80
    # Show warnings
    --tb=short
    # Disable cachedir creation warnings
    -p no:cacheprovider

# ===== Coverage Configuration =====

[coverage:run]
# Files to include in coverage
source = src, app

# Files to exclude from coverage
omit =
    */tests/*
    */test_*.py
    */__pycache__/*
    */site-packages/*
    */venv/*
    */virtualenv/*
    */.venv/*
    */migrations/*
    */manage.py
    */setup.py
    */conftest.py

# Enable branch coverage (more thorough)
branch = True

# Store coverage data in custom location
data_file = .coverage

[coverage:report]
# Ignore missing files
ignore_errors = False

# Exclude lines from coverage (e.g., debug code)
exclude_lines =
    # Standard pragma to skip lines
    pragma: no cover
    # Don't complain about missing debug code
    def __repr__
    def __str__
    # Don't complain if tests don't hit defensive assertion code
    raise AssertionError
    raise NotImplementedError
    # Don't complain if non-runnable code isn't run
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    if typing.TYPE_CHECKING:
    # Don't complain about abstract methods
    @abstractmethod
    @abc.abstractmethod
    # Don't complain about debug blocks
    if DEBUG:
    if settings.DEBUG:

# Show missing lines in coverage report
show_missing = True

# Precision for coverage percentage
precision = 2

# Sort coverage report by name
sort = Name

[coverage:html]
# Directory for HTML coverage report
directory = htmlcov

# Title for HTML coverage report
title = Test Coverage Report

[coverage:xml]
# File for XML coverage report (for CI/CD)
output = coverage.xml

# ===== Test Markers =====

markers =
    # Mark tests as slow (can be skipped with -m "not slow")
    slow: marks tests as slow (deselect with '-m "not slow"')
    # Mark integration tests
    integration: marks tests as integration tests
    # Mark unit tests
    unit: marks tests as unit tests
    # Mark tests that require database
    database: marks tests that require database connection
    # Mark tests that require network access
    network: marks tests that require network access
    # Mark tests that require specific environment
    requires_env: marks tests that require specific environment variables
    # Mark smoke tests (quick validation)
    smoke: marks tests as smoke tests
    # Mark regression tests
    regression: marks tests as regression tests
    # Mark tests that should be skipped in CI
    skip_ci: marks tests to skip in CI environment
    # Mark flaky tests (tests that sometimes fail)
    flaky: marks tests as flaky (may fail intermittently)

# ===== Logging =====

# Log level for pytest
log_cli = False
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Log file configuration
log_file = tests/logs/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s - %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# ===== Warnings =====

# Filter warnings
filterwarnings =
    # Treat all warnings as errors (strict mode)
    # error
    # Ignore specific warnings
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    # Turn specific warnings into errors
    error::UserWarning
    # Ignore warnings from third-party libraries
    ignore:.*U.*mode is deprecated:DeprecationWarning:pkg_resources

# ===== Test Collection =====

# Patterns for files/directories to ignore during test collection
norecursedirs = .git .tox .env .venv venv env dist build *.egg htmlcov node_modules

# Consider __pycache__ directories
consider_namespace_packages = False

# ===== Plugins =====

# Disable specific plugins (if needed)
# disable_test_id_escaping_and_forfeit_all_rights_to_community_support = True

# ===== Timeout =====

# Timeout for each test (requires pytest-timeout plugin)
# Uncomment to enable: timeout = 300

# Timeout method (thread or signal)
# timeout_method = thread

# ===== Fixtures =====

# Show fixtures in verbose mode
# console_output_style = progress

# ===== Async Tests =====

# Async mode for pytest-asyncio
asyncio_mode = auto

# ===== Doctest =====

# Enable doctest discovery
# addopts = --doctest-modules

# Doctest options
doctest_optionflags = NORMALIZE_WHITESPACE ELLIPSIS

# ===== JUnit XML Report (for CI/CD) =====

# Generate JUnit XML report
# junit_family = xunit2
# junit_suite_name = My Test Suite

# ===== Example Usage =====
#
# Run all tests:
#   pytest
#
# Run specific test file:
#   pytest tests/test_example.py
#
# Run specific test function:
#   pytest tests/test_example.py::test_function_name
#
# Run tests matching a pattern:
#   pytest -k "test_user"
#
# Run tests with specific marker:
#   pytest -m slow
#
# Run tests excluding a marker:
#   pytest -m "not slow"
#
# Run tests with coverage:
#   pytest --cov=src --cov-report=html
#
# Run tests in parallel (4 workers):
#   pytest -n 4
#
# Run tests with verbose output:
#   pytest -v
#
# Run tests and stop at first failure:
#   pytest -x
#
# Run tests and enter debugger on failure:
#   pytest --pdb
#
# Show test durations:
#   pytest --durations=10
#
# Generate HTML report:
#   pytest --html=report.html --self-contained-html

name: Weekly Comprehensive Validation

on:
  schedule:
    # Run every Sunday at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      include_performance_tests:
        description: 'Include performance regression tests'
        required: false
        default: true
        type: boolean
      include_deep_analysis:
        description: 'Include deep content analysis'
        required: false
        default: true
        type: boolean

env:
  VALIDATION_VERSION: "1.0"
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "18"

jobs:
  comprehensive-validation:
    name: Comprehensive Repository Validation
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1
        with:
          fetch-depth: 0  # Full history for regression analysis
          
      - name: Setup Python
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Setup Node.js
        uses: actions/setup-node@60edb5dd545a775178f52524783378180af0d1f8  # v4.0.2
        with:
          node-version: ${{ env.NODE_VERSION }}
          
      - name: Install dependencies
        run: |
          # Python dependencies for validation
          pip install --upgrade pip
          pip install pyyaml yamllint requests beautifulsoup4 tiktoken
          pip install matplotlib seaborn pandas  # For analysis
          
          # Node.js dependencies for compliance tools
          if [ -d "standards/compliance" ]; then
            cd standards/compliance
            npm install
            cd ../..
          fi
          
      - name: Run all validation tests
        id: validation-tests
        run: |
          echo "## 🔍 Comprehensive Validation Results" > validation-report.md
          echo "" >> validation-report.md
          echo "**Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> validation-report.md
          echo "**Validation Version:** ${{ env.VALIDATION_VERSION }}" >> validation-report.md
          echo "" >> validation-report.md
          
          # Track test results
          tests_passed=0
          tests_failed=0
          tests_warned=0
          
          echo "### 📋 Test Results Summary" >> validation-report.md
          echo "" >> validation-report.md
          
          # 1. Standards Consistency Validation
          echo "#### 1. Standards Consistency" >> validation-report.md
          if python scripts/validate_standards_consistency.py > standards-results.txt 2>&1; then
            echo "✅ **PASSED** - Standards schema is consistent" >> validation-report.md
            tests_passed=$((tests_passed + 1))
          else
            echo "❌ **FAILED** - Standards consistency issues found" >> validation-report.md
            echo "<details><summary>Show Details</summary>" >> validation-report.md
            echo "" >> validation-report.md
            echo '```' >> validation-report.md
            cat standards-results.txt >> validation-report.md
            echo '```' >> validation-report.md
            echo "</details>" >> validation-report.md
            tests_failed=$((tests_failed + 1))
          fi
          echo "" >> validation-report.md
          
          # 2. Cross-Reference Validation
          echo "#### 2. Cross-Reference Validation" >> validation-report.md
          if python tests/validate_cross_references.py > cross-ref-results.txt 2>&1; then
            echo "✅ **PASSED** - All cross-references are valid" >> validation-report.md
            tests_passed=$((tests_passed + 1))
          else
            echo "❌ **FAILED** - Cross-reference issues found" >> validation-report.md
            echo "<details><summary>Show Details</summary>" >> validation-report.md
            echo "" >> validation-report.md
            echo '```' >> validation-report.md
            cat cross-ref-results.txt >> validation-report.md
            echo '```' >> validation-report.md
            echo "</details>" >> validation-report.md
            tests_failed=$((tests_failed + 1))
          fi
          echo "" >> validation-report.md
          
          # 3. Knowledge Management Validation
          echo "#### 3. Knowledge Management" >> validation-report.md
          if bash tests/validate_knowledge_management.sh > km-results.txt 2>&1; then
            echo "✅ **PASSED** - Knowledge management standards met" >> validation-report.md
            tests_passed=$((tests_passed + 1))
          else
            echo "❌ **FAILED** - Knowledge management issues found" >> validation-report.md
            echo "<details><summary>Show Details</summary>" >> validation-report.md
            echo "" >> validation-report.md
            echo '```' >> validation-report.md
            cat km-results.txt >> validation-report.md
            echo '```' >> validation-report.md
            echo "</details>" >> validation-report.md
            tests_failed=$((tests_failed + 1))
          fi
          echo "" >> validation-report.md
          
          # 4. Markdown Link Validation
          echo "#### 4. Markdown Link Validation" >> validation-report.md
          if python scripts/validate_markdown_links.py > link-results.txt 2>&1; then
            echo "✅ **PASSED** - All markdown links are valid" >> validation-report.md
            tests_passed=$((tests_passed + 1))
          else
            echo "❌ **FAILED** - Broken links found" >> validation-report.md
            echo "<details><summary>Show Details</summary>" >> validation-report.md
            echo "" >> validation-report.md
            echo '```' >> validation-report.md
            cat link-results.txt >> validation-report.md
            echo '```' >> validation-report.md
            echo "</details>" >> validation-report.md
            tests_failed=$((tests_failed + 1))
          fi
          echo "" >> validation-report.md
          
          # 5. Standards Graph Validation
          echo "#### 5. Standards Graph Validation" >> validation-report.md
          if python scripts/validate_standards_graph.py > graph-results.txt 2>&1; then
            echo "✅ **PASSED** - Standards graph is valid" >> validation-report.md
            tests_passed=$((tests_passed + 1))
          else
            echo "❌ **FAILED** - Standards graph issues found" >> validation-report.md
            echo "<details><summary>Show Details</summary>" >> validation-report.md
            echo "" >> validation-report.md
            echo '```' >> validation-report.md
            cat graph-results.txt >> validation-report.md
            echo '```' >> validation-report.md
            echo "</details>" >> validation-report.md
            tests_failed=$((tests_failed + 1))
          fi
          echo "" >> validation-report.md
          
          # 6. Compliance Score Calculation
          echo "#### 6. Compliance Score" >> validation-report.md
          if compliance_output=$(python scripts/calculate_compliance_score.py 2>&1); then
            echo "✅ **PASSED** - Compliance score calculated successfully" >> validation-report.md
            echo "" >> validation-report.md
            echo "$compliance_output" | while read line; do
              echo "- $line" >> validation-report.md
            done
            tests_passed=$((tests_passed + 1))
          else
            echo "❌ **FAILED** - Compliance score calculation failed" >> validation-report.md
            tests_failed=$((tests_failed + 1))
          fi
          echo "" >> validation-report.md
          
          # Output test summary
          echo "tests_passed=$tests_passed" >> $GITHUB_OUTPUT
          echo "tests_failed=$tests_failed" >> $GITHUB_OUTPUT
          echo "tests_warned=$tests_warned" >> $GITHUB_OUTPUT
          
          # Update summary in report
          total_tests=$((tests_passed + tests_failed + tests_warned))
          echo "### 📊 Summary Statistics" >> validation-report.md
          echo "" >> validation-report.md
          echo "| Metric | Count |" >> validation-report.md
          echo "|--------|-------|" >> validation-report.md
          echo "| Tests Passed | $tests_passed |" >> validation-report.md
          echo "| Tests Failed | $tests_failed |" >> validation-report.md
          echo "| Tests with Warnings | $tests_warned |" >> validation-report.md
          echo "| Total Tests | $total_tests |" >> validation-report.md
          echo "" >> validation-report.md

  performance-regression-test:
    name: Performance Regression Testing
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: ${{ github.event.inputs.include_performance_tests != 'false' }}
    
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1
        with:
          fetch-depth: 0
          
      - name: Setup Python
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install performance testing dependencies
        run: |
          pip install --upgrade pip
          pip install psutil memory-profiler time pytest pytest-benchmark
          pip install matplotlib pandas numpy  # For performance visualization
          
      - name: Benchmark script execution times
        id: script-benchmarks
        run: |
          echo "## ⏱️ Performance Regression Test Results" > performance-report.md
          echo "" >> performance-report.md
          
          # Create benchmark script
          cat > benchmark_scripts.py << 'EOF'
          import time
          import subprocess
          import sys
          import json
          from pathlib import Path
          
          def benchmark_script(script_path, iterations=3):
              """Benchmark a script execution time"""
              times = []
              for i in range(iterations):
                  start = time.time()
                  try:
                      result = subprocess.run([sys.executable, script_path], 
                                            capture_output=True, text=True, timeout=60)
                      end = time.time()
                      if result.returncode == 0:
                          times.append(end - start)
                      else:
                          print(f"Script {script_path} failed on iteration {i+1}")
                  except subprocess.TimeoutExpired:
                      print(f"Script {script_path} timed out on iteration {i+1}")
              
              if times:
                  return {
                      'mean': sum(times) / len(times),
                      'min': min(times),
                      'max': max(times),
                      'runs': len(times)
                  }
              return None
          
          # Benchmark critical scripts
          scripts_to_test = [
              'scripts/calculate_compliance_score.py',
              'scripts/validate_standards_consistency.py',
              'scripts/validate_markdown_links.py',
              'scripts/generate_summary.py',
              'scripts/generate_reference.py'
          ]
          
          results = {}
          for script in scripts_to_test:
              if Path(script).exists():
                  print(f"Benchmarking {script}...")
                  benchmark = benchmark_script(script)
                  if benchmark:
                      results[script] = benchmark
                      print(f"  Mean: {benchmark['mean']:.2f}s")
                  else:
                      print(f"  Failed to benchmark")
              else:
                  print(f"Script {script} not found")
          
          # Save results
          with open('performance-results.json', 'w') as f:
              json.dump(results, f, indent=2)
          EOF
          
          python benchmark_scripts.py > benchmark-output.txt 2>&1
          
          # Process results and add to report
          echo "### Script Performance Benchmarks" >> performance-report.md
          echo "" >> performance-report.md
          
          if [ -f performance-results.json ]; then
            echo "| Script | Mean Time (s) | Min Time (s) | Max Time (s) |" >> performance-report.md
            echo "|--------|---------------|--------------|--------------|" >> performance-report.md
            
            python -c "
import json
with open('performance-results.json') as f:
    results = json.load(f)
    
for script, data in results.items():
    script_name = script.split('/')[-1]
    print(f'| {script_name} | {data[\"mean\"]:.2f} | {data[\"min\"]:.2f} | {data[\"max\"]:.2f} |')
" >> performance-report.md
          else
            echo "❌ Performance benchmarking failed" >> performance-report.md
          fi
          echo "" >> performance-report.md
          
      - name: Repository size trend analysis
        id: size-analysis
        run: |
          echo "### Repository Size Trend Analysis" >> performance-report.md
          echo "" >> performance-report.md
          
          # Calculate current repository metrics
          repo_size_mb=$(du -sm . | cut -f1)
          file_count=$(find . -type f | wc -l)
          md_file_count=$(find . -name "*.md" | wc -l)
          total_lines=$(find . -name "*.md" -exec wc -l {} + | tail -1 | awk '{print $1}')
          
          echo "| Metric | Current Value |" >> performance-report.md
          echo "|--------|---------------|" >> performance-report.md
          echo "| Repository Size | ${repo_size_mb}MB |" >> performance-report.md
          echo "| Total Files | $file_count |" >> performance-report.md
          echo "| Markdown Files | $md_file_count |" >> performance-report.md
          echo "| Total Lines (MD) | $total_lines |" >> performance-report.md
          echo "" >> performance-report.md
          
          # Store metrics for trend analysis
          metrics_file="weekly-metrics.json"
          current_date=$(date -u +"%Y-%m-%d")
          
          # Create or update metrics file
          if [ ! -f "$metrics_file" ]; then
            echo "[]" > "$metrics_file"
          fi
          
          # Add current metrics
          python -c "
import json
from datetime import datetime

# Load existing metrics
with open('$metrics_file', 'r') as f:
    metrics = json.load(f)

# Add current data
current_data = {
    'date': '$current_date',
    'repo_size_mb': $repo_size_mb,
    'file_count': $file_count,
    'md_file_count': $md_file_count,
    'total_lines': $total_lines
}

metrics.append(current_data)

# Keep only last 12 weeks of data
metrics = metrics[-12:]

# Save updated metrics
with open('$metrics_file', 'w') as f:
    json.dump(metrics, f, indent=2)
"
          
          echo "size_mb=$repo_size_mb" >> $GITHUB_OUTPUT
          echo "file_count=$file_count" >> $GITHUB_OUTPUT
          
      - name: Upload performance report
        uses: actions/upload-artifact@c7d193f32edcb7bfad88892161225aeda64e9392  # v4.0.0
        with:
          name: performance-regression-report
          path: performance-report.md
          retention-days: 90

  deep-content-analysis:
    name: Deep Content Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 25
    if: ${{ github.event.inputs.include_deep_analysis != 'false' }}
    
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1
        
      - name: Setup Python
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install analysis dependencies
        run: |
          pip install --upgrade pip
          pip install pyyaml textstat readability nltk pandas
          pip install matplotlib seaborn  # For visualization
          
      - name: Content quality analysis
        id: content-analysis
        run: |
          echo "## 📚 Deep Content Analysis Results" > content-analysis-report.md
          echo "" >> content-analysis-report.md
          
          # Create content analysis script
          cat > analyze_content.py << 'EOF'
          import os
          import re
          import textstat
          from pathlib import Path
          import json
          
          def analyze_markdown_file(file_path):
              """Analyze a markdown file for quality metrics"""
              try:
                  with open(file_path, 'r', encoding='utf-8') as f:
                      content = f.read()
                  
                  # Remove markdown syntax for readability analysis
                  plain_text = re.sub(r'[#*`\[\]()_-]', ' ', content)
                  plain_text = re.sub(r'\s+', ' ', plain_text).strip()
                  
                  if len(plain_text) < 100:  # Skip very short files
                      return None
                  
                  return {
                      'file_path': str(file_path),
                      'word_count': len(plain_text.split()),
                      'char_count': len(content),
                      'flesch_score': textstat.flesch_reading_ease(plain_text),
                      'flesch_grade': textstat.flesch_kincaid_grade(plain_text),
                      'avg_sentence_length': textstat.avg_sentence_length(plain_text),
                      'syllable_count': textstat.syllable_count(plain_text),
                      'difficult_words': textstat.difficult_words(plain_text)
                  }
              except Exception as e:
                  print(f"Error analyzing {file_path}: {e}")
                  return None
          
          # Analyze all markdown files
          md_files = list(Path('.').rglob('*.md'))
          results = []
          
          for md_file in md_files:
              # Skip certain directories
              if any(skip in str(md_file) for skip in ['node_modules', '.git', 'dist']):
                  continue
              
              analysis = analyze_markdown_file(md_file)
              if analysis:
                  results.append(analysis)
          
          # Calculate overall statistics
          if results:
              total_words = sum(r['word_count'] for r in results)
              avg_flesch = sum(r['flesch_score'] for r in results) / len(results)
              avg_grade = sum(r['flesch_grade'] for r in results) / len(results)
              
              stats = {
                  'total_files': len(results),
                  'total_words': total_words,
                  'avg_words_per_file': total_words / len(results),
                  'avg_flesch_score': avg_flesch,
                  'avg_grade_level': avg_grade,
                  'files_analyzed': [r['file_path'] for r in results]
              }
              
              # Save detailed results
              with open('content-analysis-results.json', 'w') as f:
                  json.dump({'stats': stats, 'file_results': results}, f, indent=2)
              
              print(f"Analyzed {len(results)} markdown files")
              print(f"Total words: {total_words:,}")
              print(f"Average readability score: {avg_flesch:.1f}")
              print(f"Average grade level: {avg_grade:.1f}")
          else:
              print("No markdown files found for analysis")
          EOF
          
          python analyze_content.py > content-analysis-output.txt 2>&1
          
          # Process results and add to report
          echo "### Content Quality Metrics" >> content-analysis-report.md
          echo "" >> content-analysis-report.md
          
          if [ -f content-analysis-results.json ]; then
            python -c "
import json

with open('content-analysis-results.json') as f:
    data = json.load(f)
    stats = data['stats']

print('| Metric | Value |')
print('|--------|-------|')
print(f'| Total Files Analyzed | {stats[\"total_files\"]} |')
print(f'| Total Words | {stats[\"total_words\"]:,} |')
print(f'| Average Words/File | {stats[\"avg_words_per_file\"]:.0f} |')
print(f'| Average Readability Score | {stats[\"avg_flesch_score\"]:.1f} |')
print(f'| Average Grade Level | {stats[\"avg_grade_level\"]:.1f} |')
" >> content-analysis-report.md
            
            echo "" >> content-analysis-report.md
            echo "### Readability Assessment" >> content-analysis-report.md
            echo "" >> content-analysis-report.md
            
            # Add readability interpretation
            python -c "
import json

with open('content-analysis-results.json') as f:
    data = json.load(f)
    avg_flesch = data['stats']['avg_flesch_score']

if avg_flesch >= 90:
    level = 'Very Easy (5th grade)'
elif avg_flesch >= 80:
    level = 'Easy (6th grade)'
elif avg_flesch >= 70:
    level = 'Fairly Easy (7th grade)'
elif avg_flesch >= 60:
    level = 'Standard (8th-9th grade)'
elif avg_flesch >= 50:
    level = 'Fairly Difficult (10th-12th grade)'
elif avg_flesch >= 30:
    level = 'Difficult (College level)'
else:
    level = 'Very Difficult (Graduate level)'

print(f'Overall readability level: **{level}**')
print()
print('**Flesch Reading Ease Scale:**')
print('- 90-100: Very Easy (5th grade)')
print('- 80-89: Easy (6th grade)')
print('- 70-79: Fairly Easy (7th grade)')
print('- 60-69: Standard (8th-9th grade)')
print('- 50-59: Fairly Difficult (10th-12th grade)')
print('- 30-49: Difficult (College level)')
print('- 0-29: Very Difficult (Graduate level)')
" >> content-analysis-report.md
          else
            echo "❌ Content analysis failed" >> content-analysis-report.md
          fi
          
          echo "" >> content-analysis-report.md
          
      - name: Upload content analysis report
        uses: actions/upload-artifact@c7d193f32edcb7bfad88892161225aeda64e9392  # v4.0.0
        with:
          name: deep-content-analysis-report
          path: content-analysis-report.md
          retention-days: 90

  comprehensive-summary:
    name: Weekly Validation Summary
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [comprehensive-validation, performance-regression-test, deep-content-analysis]
    if: always()
    
    steps:
      - name: Download all reports
        uses: actions/download-artifact@c7d193f32edcb7bfad88892161225aeda64e9392  # v4.0.0
        with:
          path: weekly-reports
          
      - name: Create comprehensive summary
        id: create-summary
        run: |
          echo "# 📊 Weekly Comprehensive Validation Summary" > weekly-summary.md
          echo "" >> weekly-summary.md
          echo "**Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> weekly-summary.md
          echo "**Validation Version:** ${{ env.VALIDATION_VERSION }}" >> weekly-summary.md
          echo "" >> weekly-summary.md
          
          # Job status summary
          echo "## 🎯 Execution Summary" >> weekly-summary.md
          echo "" >> weekly-summary.md
          echo "| Job | Status | Duration |" >> weekly-summary.md
          echo "|-----|--------|----------|" >> weekly-summary.md
          echo "| Comprehensive Validation | ${{ needs.comprehensive-validation.result }} | - |" >> weekly-summary.md
          echo "| Performance Regression Test | ${{ needs.performance-regression-test.result }} | - |" >> weekly-summary.md
          echo "| Deep Content Analysis | ${{ needs.deep-content-analysis.result }} | - |" >> weekly-summary.md
          echo "" >> weekly-summary.md
          
          # Validation results
          if [ "${{ needs.comprehensive-validation.result }}" = "success" ]; then
            tests_passed="${{ needs.comprehensive-validation.outputs.tests_passed }}"
            tests_failed="${{ needs.comprehensive-validation.outputs.tests_failed }}"
            
            echo "## ✅ Validation Test Results" >> weekly-summary.md
            echo "" >> weekly-summary.md
            echo "- **Tests Passed:** $tests_passed" >> weekly-summary.md
            echo "- **Tests Failed:** $tests_failed" >> weekly-summary.md
            echo "" >> weekly-summary.md
            
            if [ "$tests_failed" -gt 0 ]; then
              echo "⚠️ **Action Required:** $tests_failed validation tests failed and require immediate attention." >> weekly-summary.md
              echo "" >> weekly-summary.md
            fi
          else
            echo "## ❌ Validation Test Results" >> weekly-summary.md
            echo "" >> weekly-summary.md
            echo "Comprehensive validation job failed. Check logs for details." >> weekly-summary.md
            echo "" >> weekly-summary.md
          fi
          
          # Performance results
          if [ "${{ needs.performance-regression-test.result }}" = "success" ]; then
            repo_size="${{ needs.performance-regression-test.outputs.size_mb }}"
            echo "## 📈 Performance Metrics" >> weekly-summary.md
            echo "" >> weekly-summary.md
            echo "- **Repository Size:** ${repo_size}MB" >> weekly-summary.md
            echo "- **Performance Tests:** Completed successfully" >> weekly-summary.md
            echo "" >> weekly-summary.md
          elif [ "${{ github.event.inputs.include_performance_tests }}" != "false" ]; then
            echo "## ❌ Performance Metrics" >> weekly-summary.md
            echo "" >> weekly-summary.md
            echo "Performance regression tests failed. Check logs for details." >> weekly-summary.md
            echo "" >> weekly-summary.md
          fi
          
          # Content analysis results
          if [ "${{ needs.deep-content-analysis.result }}" = "success" ]; then
            echo "## 📚 Content Quality" >> weekly-summary.md
            echo "" >> weekly-summary.md
            echo "- **Content Analysis:** Completed successfully" >> weekly-summary.md
            echo "- **Detailed Report:** Available in artifacts" >> weekly-summary.md
            echo "" >> weekly-summary.md
          elif [ "${{ github.event.inputs.include_deep_analysis }}" != "false" ]; then
            echo "## ❌ Content Quality" >> weekly-summary.md
            echo "" >> weekly-summary.md
            echo "Deep content analysis failed. Check logs for details." >> weekly-summary.md
            echo "" >> weekly-summary.md
          fi
          
          echo "## 📋 Detailed Reports" >> weekly-summary.md
          echo "" >> weekly-summary.md
          echo "Detailed reports for each validation component are available as workflow artifacts:" >> weekly-summary.md
          echo "" >> weekly-summary.md
          echo "- **Validation Report:** Comprehensive validation results and test details" >> weekly-summary.md
          echo "- **Performance Report:** Script benchmarks and repository metrics" >> weekly-summary.md
          echo "- **Content Analysis:** Content quality and readability analysis" >> weekly-summary.md
          echo "" >> weekly-summary.md
          
          echo "## 🔧 Recommended Actions" >> weekly-summary.md
          echo "" >> weekly-summary.md
          echo "1. Review all failed validation tests and address issues immediately" >> weekly-summary.md
          echo "2. Monitor performance trends and investigate any regressions" >> weekly-summary.md
          echo "3. Review content quality metrics and improve documentation where needed" >> weekly-summary.md
          echo "4. Schedule additional validation runs if critical issues are found" >> weekly-summary.md
          echo "" >> weekly-summary.md
          
          echo "## 📊 Next Review" >> weekly-summary.md
          echo "" >> weekly-summary.md
          echo "- **Next Weekly Validation:** $(date -d '+7 days' -u +'%Y-%m-%d')" >> weekly-summary.md
          echo "- **Next Monthly Analysis:** $(date -d 'next month' -u +'%Y-%m-01')" >> weekly-summary.md
          echo "" >> weekly-summary.md
          
          # Determine overall status
          overall_status="success"
          if [ "${{ needs.comprehensive-validation.result }}" != "success" ] || 
             [ "${{ needs.performance-regression-test.result }}" = "failure" ] || 
             [ "${{ needs.deep-content-analysis.result }}" = "failure" ]; then
            overall_status="warning"
          fi
          
          if [ "${{ needs.comprehensive-validation.outputs.tests_failed }}" -gt 0 ]; then
            overall_status="critical"
          fi
          
          echo "overall_status=$overall_status" >> $GITHUB_OUTPUT
          
      - name: Upload weekly summary
        uses: actions/upload-artifact@c7d193f32edcb7bfad88892161225aeda64e9392  # v4.0.0
        with:
          name: weekly-validation-summary
          path: weekly-summary.md
          retention-days: 365  # Keep weekly summaries for a year
          
      - name: Update step summary
        run: |
          cat weekly-summary.md >> $GITHUB_STEP_SUMMARY
          
      - name: Create issue for critical validation failures
        if: steps.create-summary.outputs.overall_status == 'critical'
        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea  # v7.0.1
        with:
          script: |
            const title = `🚨 Critical Weekly Validation Failures - ${new Date().toISOString().split('T')[0]}`;
            const body = `
            ## Critical Validation Failures Detected
            
            The weekly comprehensive validation has detected critical failures that require immediate attention.
            
            **Failed Tests:** ${{ needs.comprehensive-validation.outputs.tests_failed }}
            **Validation Job Status:** ${{ needs.comprehensive-validation.result }}
            
            **Run Details:**
            - Workflow: ${{ github.workflow }}
            - Run ID: ${{ github.run_id }}
            - Date: ${new Date().toUTCString()}
            
            ### Immediate Actions Required:
            1. Download and review the comprehensive validation report from artifacts
            2. Address all failed validation tests immediately
            3. Run targeted validation after fixes
            4. Confirm fixes with next daily health check
            
            **Workflow Link:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            
            This issue was automatically created by the weekly validation workflow.
            `;
            
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['validation', 'critical', 'automated', 'weekly']
            });
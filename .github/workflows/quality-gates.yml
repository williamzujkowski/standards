name: Quality Gates

on:
  pull_request:
    branches: [main, master, develop]
    types: [opened, synchronize, reopened, ready_for_review]
  push:
    branches: [main, master]
  workflow_dispatch:
    inputs:
      strict_mode:
        description: 'Enable strict quality enforcement'
        required: false
        default: false
        type: boolean

env:
  QUALITY_GATES_VERSION: "1.0"
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "18"

jobs:
  quality-gate-prerequisites:
    name: Quality Gate Prerequisites
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    outputs:
      changed_files: ${{ steps.changed-files.outputs.files }}
      has_md_changes: ${{ steps.changed-files.outputs.has_md_changes }}
      has_config_changes: ${{ steps.changed-files.outputs.has_config_changes }}
      has_script_changes: ${{ steps.changed-files.outputs.has_script_changes }}
      quality_threshold: ${{ steps.set-threshold.outputs.threshold }}
      
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1
        with:
          fetch-depth: 0
          
      - name: Get changed files
        id: changed-files
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            # Get files changed in PR
            changed_files=$(git diff --name-only origin/${{ github.base_ref }}...HEAD)
          else
            # Get files changed in last commit
            changed_files=$(git diff --name-only HEAD~1...HEAD)
          fi
          
          echo "Changed files:"
          echo "$changed_files"
          
          # Check for different types of changes
          has_md_changes="false"
          has_config_changes="false"
          has_script_changes="false"
          
          while IFS= read -r file; do
            if [[ "$file" == *.md ]]; then
              has_md_changes="true"
            elif [[ "$file" == config/* ]] || [[ "$file" == *.yaml ]] || [[ "$file" == *.yml ]] || [[ "$file" == *.json ]]; then
              has_config_changes="true"
            elif [[ "$file" == scripts/* ]] || [[ "$file" == tests/* ]] || [[ "$file" == *.py ]] || [[ "$file" == *.sh ]]; then
              has_script_changes="true"
            fi
          done <<< "$changed_files"
          
          # Output results
          echo "files<<EOF" >> $GITHUB_OUTPUT
          echo "$changed_files" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          echo "has_md_changes=$has_md_changes" >> $GITHUB_OUTPUT
          echo "has_config_changes=$has_config_changes" >> $GITHUB_OUTPUT
          echo "has_script_changes=$has_script_changes" >> $GITHUB_OUTPUT
          
      - name: Set quality threshold
        id: set-threshold
        run: |
          # Set quality threshold based on strict mode and context
          if [ "${{ github.event.inputs.strict_mode }}" = "true" ]; then
            threshold="95"
          elif [ "${{ github.event_name }}" = "push" ] && [ "${{ github.ref }}" = "refs/heads/main" ]; then
            threshold="90"  # Higher threshold for main branch
          else
            threshold="85"  # Standard threshold for PRs
          fi
          
          echo "Quality threshold set to: $threshold%"
          echo "threshold=$threshold" >> $GITHUB_OUTPUT

  syntax-and-format-validation:
    name: Syntax & Format Validation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: quality-gate-prerequisites
    
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1
        
      - name: Setup Python
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Setup Node.js
        uses: actions/setup-node@60edb5dd545a775178f52524783378180af0d1f8  # v4.0.2
        with:
          node-version: ${{ env.NODE_VERSION }}
          
      - name: Install validation tools
        run: |
          pip install --upgrade pip
          pip install yamllint pylint black isort flake8
          pip install markdownlint-cli2
          npm install -g markdownlint-cli
          
      - name: Validate YAML syntax
        if: needs.quality-gate-prerequisites.outputs.has_config_changes == 'true'
        run: |
          echo "## 📋 YAML Validation Results" > syntax-validation-report.md
          echo "" >> syntax-validation-report.md
          
          yaml_errors=0
          yaml_files=$(echo "${{ needs.quality-gate-prerequisites.outputs.changed_files }}" | grep -E '\.(yaml|yml)$' || true)
          
          if [ -n "$yaml_files" ]; then
            echo "### YAML Files Validated" >> syntax-validation-report.md
            echo "" >> syntax-validation-report.md
            
            while IFS= read -r yaml_file; do
              if [ -f "$yaml_file" ]; then
                echo "Validating $yaml_file..."
                if yamllint "$yaml_file" > yaml_lint_output.txt 2>&1; then
                  echo "- ✅ $yaml_file: Valid" >> syntax-validation-report.md
                else
                  echo "- ❌ $yaml_file: Invalid" >> syntax-validation-report.md
                  echo "  <details><summary>Errors</summary>" >> syntax-validation-report.md
                  echo "" >> syntax-validation-report.md
                  echo "  \`\`\`" >> syntax-validation-report.md
                  cat yaml_lint_output.txt >> syntax-validation-report.md
                  echo "  \`\`\`" >> syntax-validation-report.md
                  echo "  </details>" >> syntax-validation-report.md
                  yaml_errors=$((yaml_errors + 1))
                fi
              fi
            done <<< "$yaml_files"
          else
            echo "No YAML files to validate." >> syntax-validation-report.md
          fi
          echo "" >> syntax-validation-report.md
          
          echo "yaml_errors=$yaml_errors" >> $GITHUB_ENV
          
      - name: Validate JSON syntax
        if: needs.quality-gate-prerequisites.outputs.has_config_changes == 'true'
        run: |
          echo "### JSON Files Validated" >> syntax-validation-report.md
          echo "" >> syntax-validation-report.md
          
          json_errors=0
          json_files=$(echo "${{ needs.quality-gate-prerequisites.outputs.changed_files }}" | grep '\.json$' || true)
          
          if [ -n "$json_files" ]; then
            while IFS= read -r json_file; do
              if [ -f "$json_file" ]; then
                echo "Validating $json_file..."
                if python -m json.tool "$json_file" > /dev/null 2>&1; then
                  echo "- ✅ $json_file: Valid JSON" >> syntax-validation-report.md
                else
                  echo "- ❌ $json_file: Invalid JSON" >> syntax-validation-report.md
                  json_errors=$((json_errors + 1))
                fi
              fi
            done <<< "$json_files"
          else
            echo "No JSON files to validate." >> syntax-validation-report.md
          fi
          echo "" >> syntax-validation-report.md
          
          echo "json_errors=$json_errors" >> $GITHUB_ENV
          
      - name: Validate Python syntax
        if: needs.quality-gate-prerequisites.outputs.has_script_changes == 'true'
        run: |
          echo "### Python Files Validated" >> syntax-validation-report.md
          echo "" >> syntax-validation-report.md
          
          python_errors=0
          python_files=$(echo "${{ needs.quality-gate-prerequisites.outputs.changed_files }}" | grep '\.py$' || true)
          
          if [ -n "$python_files" ]; then
            while IFS= read -r py_file; do
              if [ -f "$py_file" ]; then
                echo "Validating $py_file..."
                if python -m py_compile "$py_file" 2>/dev/null; then
                  echo "- ✅ $py_file: Valid Python syntax" >> syntax-validation-report.md
                else
                  echo "- ❌ $py_file: Python syntax error" >> syntax-validation-report.md
                  python_errors=$((python_errors + 1))
                fi
              fi
            done <<< "$python_files"
          else
            echo "No Python files to validate." >> syntax-validation-report.md
          fi
          echo "" >> syntax-validation-report.md
          
          echo "python_errors=$python_errors" >> $GITHUB_ENV
          
      - name: Validate Markdown format
        if: needs.quality-gate-prerequisites.outputs.has_md_changes == 'true'
        run: |
          echo "### Markdown Files Validated" >> syntax-validation-report.md
          echo "" >> syntax-validation-report.md
          
          markdown_errors=0
          md_files=$(echo "${{ needs.quality-gate-prerequisites.outputs.changed_files }}" | grep '\.md$' || true)
          
          if [ -n "$md_files" ]; then
            while IFS= read -r md_file; do
              if [ -f "$md_file" ]; then
                echo "Validating $md_file..."
                # Basic markdown validation
                if markdownlint "$md_file" > md_lint_output.txt 2>&1; then
                  echo "- ✅ $md_file: Valid Markdown format" >> syntax-validation-report.md
                else
                  echo "- ⚠️ $md_file: Markdown formatting issues" >> syntax-validation-report.md
                  echo "  <details><summary>Issues</summary>" >> syntax-validation-report.md
                  echo "" >> syntax-validation-report.md
                  echo "  \`\`\`" >> syntax-validation-report.md
                  head -10 md_lint_output.txt >> syntax-validation-report.md
                  echo "  \`\`\`" >> syntax-validation-report.md
                  echo "  </details>" >> syntax-validation-report.md
                  # Don't count as hard error for markdown formatting
                fi
              fi
            done <<< "$md_files"
          else
            echo "No Markdown files to validate." >> syntax-validation-report.md
          fi
          echo "" >> syntax-validation-report.md
          
      - name: Calculate syntax validation score
        id: syntax-score
        run: |
          total_errors=$((${yaml_errors:-0} + ${json_errors:-0} + ${python_errors:-0}))
          total_files=$(echo "${{ needs.quality-gate-prerequisites.outputs.changed_files }}" | wc -l)
          
          if [ "$total_files" -eq 0 ]; then
            score=100
          else
            # Calculate score: 100% - (errors/files * 100)
            score=$((100 - (total_errors * 100 / total_files)))
            score=$((score < 0 ? 0 : score))
          fi
          
          echo "### 📊 Syntax Validation Summary" >> syntax-validation-report.md
          echo "" >> syntax-validation-report.md
          echo "| Metric | Value |" >> syntax-validation-report.md
          echo "|--------|-------|" >> syntax-validation-report.md
          echo "| Total Files Checked | $total_files |" >> syntax-validation-report.md
          echo "| Total Errors | $total_errors |" >> syntax-validation-report.md
          echo "| Syntax Score | $score% |" >> syntax-validation-report.md
          echo "" >> syntax-validation-report.md
          
          if [ "$total_errors" -gt 0 ]; then
            echo "❌ **Syntax validation failed with $total_errors errors**" >> syntax-validation-report.md
          else
            echo "✅ **All syntax validation checks passed**" >> syntax-validation-report.md
          fi
          
          echo "syntax_score=$score" >> $GITHUB_OUTPUT
          echo "syntax_errors=$total_errors" >> $GITHUB_OUTPUT
          
      - name: Upload syntax validation report
        uses: actions/upload-artifact@c7d193f32edcb7bfad88892161225aeda64e9392  # v4.0.0
        with:
          name: syntax-validation-report
          path: syntax-validation-report.md
          retention-days: 30

  content-quality-validation:
    name: Content Quality Validation
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: quality-gate-prerequisites
    if: needs.quality-gate-prerequisites.outputs.has_md_changes == 'true'
    
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1
        
      - name: Setup Python
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install content analysis tools
        run: |
          pip install --upgrade pip
          pip install textstat readability pyyaml requests beautifulsoup4
          
      - name: Validate content standards
        id: content-validation
        run: |
          echo "## 📚 Content Quality Validation" > content-quality-report.md
          echo "" >> content-quality-report.md
          
          # Create content quality validation script
          cat > validate_content_quality.py << 'EOF'
          import os
          import re
          import textstat
          from pathlib import Path
          import json
          
          def validate_markdown_file(file_path, changed_files):
              """Validate a markdown file for quality standards"""
              if str(file_path) not in changed_files:
                  return None  # Skip unchanged files
                  
              try:
                  with open(file_path, 'r', encoding='utf-8') as f:
                      content = f.read()
              except:
                  return None
              
              issues = []
              warnings = []
              score = 100
              
              # Basic content checks
              if len(content) < 100:
                  issues.append("File is too short (< 100 characters)")
                  score -= 30
              
              if not re.search(r'^#\s+', content, re.MULTILINE):
                  issues.append("No main heading (H1) found")
                  score -= 20
              
              # Check for table of contents in longer files
              if len(content) > 2000 and 'table of contents' not in content.lower() and 'toc' not in content.lower():
                  warnings.append("Long file without table of contents")
                  score -= 5
              
              # Check for links
              links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
              if len(content) > 500 and len(links) == 0:
                  warnings.append("No links found in substantial content")
                  score -= 10
              
              # Check for code examples in standards files
              if '_STANDARDS.md' in str(file_path):
                  code_blocks = re.findall(r'```[\s\S]*?```', content)
                  if len(code_blocks) == 0:
                      warnings.append("Standards file without code examples")
                      score -= 10
              
              # Readability analysis
              plain_text = re.sub(r'[#*`\[\]()_-]', ' ', content)
              plain_text = re.sub(r'\s+', ' ', plain_text).strip()
              
              if len(plain_text) > 100:
                  flesch_score = textstat.flesch_reading_ease(plain_text)
                  if flesch_score < 30:  # Very difficult
                      warnings.append(f"Very difficult to read (Flesch score: {flesch_score:.1f})")
                      score -= 15
                  elif flesch_score < 50:  # Difficult
                      warnings.append(f"Difficult to read (Flesch score: {flesch_score:.1f})")
                      score -= 5
              
              # Structure validation
              headings = re.findall(r'^(#{1,6})\s+(.+)$', content, re.MULTILINE)
              if len(headings) > 1:
                  # Check heading hierarchy
                  prev_level = 0
                  for heading in headings:
                      current_level = len(heading[0])
                      if prev_level > 0 and current_level > prev_level + 1:
                          warnings.append("Heading hierarchy violation (skipped levels)")
                          score -= 5
                          break
                      prev_level = current_level
              
              return {
                  'file': str(file_path),
                  'score': max(0, score),
                  'issues': issues,
                  'warnings': warnings,
                  'word_count': len(content.split()),
                  'readability_score': flesch_score if len(plain_text) > 100 else None
              }
          
          # Get changed markdown files
          changed_files = """${{ needs.quality-gate-prerequisites.outputs.changed_files }}""".strip().split('\n')
          md_files = [f for f in changed_files if f.endswith('.md') and f]
          
          results = []
          total_score = 0
          total_files = 0
          
          for md_file in md_files:
              if os.path.exists(md_file):
                  result = validate_markdown_file(Path(md_file), changed_files)
                  if result:
                      results.append(result)
                      total_score += result['score']
                      total_files += 1
          
          # Calculate average score
          avg_score = (total_score / total_files) if total_files > 0 else 100
          
          # Save results
          validation_results = {
              'average_score': round(avg_score, 1),
              'total_files': total_files,
              'file_results': results
          }
          
          with open('content-validation-results.json', 'w') as f:
              json.dump(validation_results, f, indent=2)
          
          print(f"Content validation completed")
          print(f"Files analyzed: {total_files}")
          print(f"Average score: {avg_score:.1f}%")
          
          # Generate detailed report
          for result in results:
              print(f"\n{result['file']}: {result['score']}/100")
              if result['issues']:
                  print("  Issues:", ', '.join(result['issues']))
              if result['warnings']:
                  print("  Warnings:", ', '.join(result['warnings']))
          EOF
          
          python validate_content_quality.py > content-validation.log 2>&1
          
          # Process results into report
          if [ -f content-validation-results.json ]; then
            echo "### 📊 Content Quality Results" >> content-quality-report.md
            echo "" >> content-quality-report.md
            
            avg_score=$(python -c "import json; data=json.load(open('content-validation-results.json')); print(data['average_score'])")
            total_files=$(python -c "import json; data=json.load(open('content-validation-results.json')); print(data['total_files'])")
            
            echo "| Metric | Value |" >> content-quality-report.md
            echo "|--------|-------|" >> content-quality-report.md
            echo "| Files Analyzed | $total_files |" >> content-quality-report.md
            echo "| Average Quality Score | $avg_score% |" >> content-quality-report.md
            echo "" >> content-quality-report.md
            
            echo "### 📋 Detailed Results" >> content-quality-report.md
            echo "" >> content-quality-report.md
            
            python -c "
import json
with open('content-validation-results.json') as f:
    data = json.load(f)

for result in data['file_results']:
    print(f'#### {result[\"file\"]}')
    print()
    print(f'**Score:** {result[\"score\"]}/100')
    print(f'**Word Count:** {result[\"word_count\"]}')
    if result['readability_score']:
        print(f'**Readability Score:** {result[\"readability_score\"]:.1f}')
    print()
    
    if result['issues']:
        print('**Issues:**')
        for issue in result['issues']:
            print(f'- ❌ {issue}')
        print()
    
    if result['warnings']:
        print('**Warnings:**')
        for warning in result['warnings']:
            print(f'- ⚠️ {warning}')
        print()
" >> content-quality-report.md
            
            echo "content_score=$avg_score" >> $GITHUB_OUTPUT
          else
            echo "❌ Content validation failed" >> content-quality-report.md
            echo "content_score=0" >> $GITHUB_OUTPUT
          fi
          
      - name: Upload content quality report
        uses: actions/upload-artifact@c7d193f32edcb7bfad88892161225aeda64e9392  # v4.0.0
        with:
          name: content-quality-report
          path: content-quality-report.md
          retention-days: 30

  standards-compliance-validation:
    name: Standards Compliance Validation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: quality-gate-prerequisites
    
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1
        
      - name: Setup Python
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install pyyaml requests beautifulsoup4
          
      - name: Run standards validation
        id: standards-validation
        run: |
          echo "## 🎯 Standards Compliance Validation" > standards-compliance-report.md
          echo "" >> standards-compliance-report.md
          
          validation_score=100
          validation_errors=0
          
          # Run existing validation scripts
          echo "### Standards Consistency Check" >> standards-compliance-report.md
          echo "" >> standards-compliance-report.md
          
          if [ -f "scripts/validate_standards_consistency.py" ]; then
            if python scripts/validate_standards_consistency.py > consistency-results.txt 2>&1; then
              echo "✅ Standards consistency validation passed" >> standards-compliance-report.md
            else
              echo "❌ Standards consistency validation failed" >> standards-compliance-report.md
              echo "<details><summary>Details</summary>" >> standards-compliance-report.md
              echo "" >> standards-compliance-report.md
              echo '```' >> standards-compliance-report.md
              cat consistency-results.txt >> standards-compliance-report.md
              echo '```' >> standards-compliance-report.md
              echo "</details>" >> standards-compliance-report.md
              validation_score=$((validation_score - 25))
              validation_errors=$((validation_errors + 1))
            fi
          else
            echo "⚠️ Standards consistency script not found" >> standards-compliance-report.md
            validation_score=$((validation_score - 10))
          fi
          echo "" >> standards-compliance-report.md
          
          # Cross-reference validation
          echo "### Cross-Reference Validation" >> standards-compliance-report.md
          echo "" >> standards-compliance-report.md
          
          if [ -f "tests/validate_cross_references.py" ]; then
            if python tests/validate_cross_references.py > cross-ref-results.txt 2>&1; then
              echo "✅ Cross-reference validation passed" >> standards-compliance-report.md
            else
              echo "❌ Cross-reference validation failed" >> standards-compliance-report.md
              echo "<details><summary>Details</summary>" >> standards-compliance-report.md
              echo "" >> standards-compliance-report.md
              echo '```' >> standards-compliance-report.md
              cat cross-ref-results.txt >> standards-compliance-report.md
              echo '```' >> standards-compliance-report.md
              echo "</details>" >> standards-compliance-report.md
              validation_score=$((validation_score - 25))
              validation_errors=$((validation_errors + 1))
            fi
          else
            echo "⚠️ Cross-reference validation script not found" >> standards-compliance-report.md
            validation_score=$((validation_score - 10))
          fi
          echo "" >> standards-compliance-report.md
          
          # Markdown link validation
          echo "### Markdown Link Validation" >> standards-compliance-report.md
          echo "" >> standards-compliance-report.md
          
          if [ -f "scripts/validate_markdown_links.py" ]; then
            if python scripts/validate_markdown_links.py > link-results.txt 2>&1; then
              echo "✅ Markdown link validation passed" >> standards-compliance-report.md
            else
              echo "❌ Markdown link validation failed" >> standards-compliance-report.md
              echo "<details><summary>Details</summary>" >> standards-compliance-report.md
              echo "" >> standards-compliance-report.md
              echo '```' >> standards-compliance-report.md
              head -20 link-results.txt >> standards-compliance-report.md
              echo '```' >> standards-compliance-report.md
              echo "</details>" >> standards-compliance-report.md
              validation_score=$((validation_score - 20))
              validation_errors=$((validation_errors + 1))
            fi
          else
            echo "⚠️ Markdown link validation script not found" >> standards-compliance-report.md
            validation_score=$((validation_score - 10))
          fi
          echo "" >> standards-compliance-report.md
          
          # Compliance score calculation
          echo "### Compliance Score Calculation" >> standards-compliance-report.md
          echo "" >> standards-compliance-report.md
          
          if [ -f "scripts/calculate_compliance_score.py" ]; then
            if compliance_output=$(python scripts/calculate_compliance_score.py 2>&1); then
              echo "✅ Compliance score calculated successfully" >> standards-compliance-report.md
              echo "" >> standards-compliance-report.md
              echo "$compliance_output" | while read line; do
                echo "- $line" >> standards-compliance-report.md
              done
            else
              echo "❌ Compliance score calculation failed" >> standards-compliance-report.md
              validation_score=$((validation_score - 15))
              validation_errors=$((validation_errors + 1))
            fi
          else
            echo "⚠️ Compliance score script not found" >> standards-compliance-report.md
            validation_score=$((validation_score - 10))
          fi
          echo "" >> standards-compliance-report.md
          
          # Final validation summary
          echo "### 📊 Standards Validation Summary" >> standards-compliance-report.md
          echo "" >> standards-compliance-report.md
          echo "| Metric | Value |" >> standards-compliance-report.md
          echo "|--------|-------|" >> standards-compliance-report.md
          echo "| Validation Score | $validation_score% |" >> standards-compliance-report.md
          echo "| Validation Errors | $validation_errors |" >> standards-compliance-report.md
          echo "" >> standards-compliance-report.md
          
          if [ "$validation_errors" -eq 0 ]; then
            echo "✅ **All standards validation checks passed**" >> standards-compliance-report.md
          else
            echo "❌ **Standards validation failed with $validation_errors errors**" >> standards-compliance-report.md
          fi
          
          echo "standards_score=$validation_score" >> $GITHUB_OUTPUT
          echo "standards_errors=$validation_errors" >> $GITHUB_OUTPUT
          
      - name: Upload standards compliance report
        uses: actions/upload-artifact@c7d193f32edcb7bfad88892161225aeda64e9392  # v4.0.0
        with:
          name: standards-compliance-report
          path: standards-compliance-report.md
          retention-days: 30

  quality-gate-decision:
    name: Quality Gate Decision
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [quality-gate-prerequisites, syntax-and-format-validation, content-quality-validation, standards-compliance-validation]
    if: always()
    
    steps:
      - name: Calculate overall quality score
        id: calculate-score
        run: |
          # Get individual scores
          syntax_score="${{ needs.syntax-and-format-validation.outputs.syntax_score || '100' }}"
          content_score="${{ needs.content-quality-validation.outputs.content_score || '100' }}"
          standards_score="${{ needs.standards-compliance-validation.outputs.standards_score || '100' }}"
          
          # Calculate weighted average
          # Syntax: 30%, Content: 30%, Standards: 40%
          overall_score=$(( (syntax_score * 30 + content_score * 30 + standards_score * 40) / 100 ))
          
          echo "Individual scores:"
          echo "- Syntax & Format: $syntax_score%"
          echo "- Content Quality: $content_score%"
          echo "- Standards Compliance: $standards_score%"
          echo "Overall Quality Score: $overall_score%"
          
          # Get quality threshold
          threshold="${{ needs.quality-gate-prerequisites.outputs.quality_threshold }}"
          echo "Quality Threshold: $threshold%"
          
          # Determine if quality gate passes
          if [ "$overall_score" -ge "$threshold" ]; then
            gate_result="PASS"
            gate_status="success"
          else
            gate_result="FAIL"
            gate_status="failure"
          fi
          
          echo "Quality Gate Result: $gate_result"
          
          # Set outputs
          echo "overall_score=$overall_score" >> $GITHUB_OUTPUT
          echo "gate_result=$gate_result" >> $GITHUB_OUTPUT
          echo "gate_status=$gate_status" >> $GITHUB_OUTPUT
          echo "threshold=$threshold" >> $GITHUB_OUTPUT
          
      - name: Generate quality gate summary
        id: generate-summary
        run: |
          echo "# 🚪 Quality Gate Results" > quality-gate-summary.md
          echo "" >> quality-gate-summary.md
          echo "**Overall Score:** ${{ steps.calculate-score.outputs.overall_score }}%" >> quality-gate-summary.md
          echo "**Threshold:** ${{ steps.calculate-score.outputs.threshold }}%" >> quality-gate-summary.md
          echo "**Result:** ${{ steps.calculate-score.outputs.gate_result }}" >> quality-gate-summary.md
          echo "" >> quality-gate-summary.md
          
          # Add status indicator
          if [ "${{ steps.calculate-score.outputs.gate_result }}" = "PASS" ]; then
            echo "## ✅ Quality Gate PASSED" >> quality-gate-summary.md
            echo "" >> quality-gate-summary.md
            echo "All quality standards have been met. This change is approved for merge." >> quality-gate-summary.md
          else
            echo "## ❌ Quality Gate FAILED" >> quality-gate-summary.md
            echo "" >> quality-gate-summary.md
            echo "Quality standards have not been met. Please address the issues below before merging." >> quality-gate-summary.md
          fi
          echo "" >> quality-gate-summary.md
          
          # Add detailed scores
          echo "## 📊 Detailed Scores" >> quality-gate-summary.md
          echo "" >> quality-gate-summary.md
          echo "| Component | Score | Weight | Status |" >> quality-gate-summary.md
          echo "|-----------|-------|--------|--------|" >> quality-gate-summary.md
          
          syntax_score="${{ needs.syntax-and-format-validation.outputs.syntax_score || '100' }}"
          content_score="${{ needs.content-quality-validation.outputs.content_score || '100' }}"
          standards_score="${{ needs.standards-compliance-validation.outputs.standards_score || '100' }}"
          
          # Status indicators
          syntax_status=$([ "$syntax_score" -ge 85 ] && echo "✅" || echo "❌")
          content_status=$([ "$content_score" -ge 85 ] && echo "✅" || echo "❌")
          standards_status=$([ "$standards_score" -ge 85 ] && echo "✅" || echo "❌")
          
          echo "| Syntax & Format | $syntax_score% | 30% | $syntax_status |" >> quality-gate-summary.md
          echo "| Content Quality | $content_score% | 30% | $content_status |" >> quality-gate-summary.md
          echo "| Standards Compliance | $standards_score% | 40% | $standards_status |" >> quality-gate-summary.md
          echo "" >> quality-gate-summary.md
          
          # Add remediation guidance
          if [ "${{ steps.calculate-score.outputs.gate_result }}" = "FAIL" ]; then
            echo "## 🔧 Remediation Guidance" >> quality-gate-summary.md
            echo "" >> quality-gate-summary.md
            echo "To pass the quality gate, please address the following:" >> quality-gate-summary.md
            echo "" >> quality-gate-summary.md
            
            if [ "$syntax_score" -lt 85 ]; then
              echo "### Syntax & Format Issues" >> quality-gate-summary.md
              echo "- Review syntax validation report for specific errors" >> quality-gate-summary.md
              echo "- Fix YAML, JSON, and Python syntax errors" >> quality-gate-summary.md
              echo "- Ensure proper file formatting" >> quality-gate-summary.md
              echo "" >> quality-gate-summary.md
            fi
            
            if [ "$content_score" -lt 85 ]; then
              echo "### Content Quality Issues" >> quality-gate-summary.md
              echo "- Review content quality report for specific issues" >> quality-gate-summary.md
              echo "- Improve readability and structure" >> quality-gate-summary.md
              echo "- Add missing headings, links, or examples" >> quality-gate-summary.md
              echo "" >> quality-gate-summary.md
            fi
            
            if [ "$standards_score" -lt 85 ]; then
              echo "### Standards Compliance Issues" >> quality-gate-summary.md
              echo "- Review standards compliance report for specific failures" >> quality-gate-summary.md
              echo "- Fix cross-reference and consistency issues" >> quality-gate-summary.md
              echo "- Ensure all validation scripts pass" >> quality-gate-summary.md
              echo "" >> quality-gate-summary.md
            fi
          fi
          
          echo "## 📋 Detailed Reports" >> quality-gate-summary.md
          echo "" >> quality-gate-summary.md
          echo "Detailed reports are available as workflow artifacts:" >> quality-gate-summary.md
          echo "" >> quality-gate-summary.md
          echo "- **Syntax Validation Report**: Detailed syntax and format validation results" >> quality-gate-summary.md
          echo "- **Content Quality Report**: Content analysis and quality assessment" >> quality-gate-summary.md
          echo "- **Standards Compliance Report**: Standards validation and compliance check results" >> quality-gate-summary.md
          echo "" >> quality-gate-summary.md
          
      - name: Upload quality gate summary
        uses: actions/upload-artifact@c7d193f32edcb7bfad88892161225aeda64e9392  # v4.0.0
        with:
          name: quality-gate-summary
          path: quality-gate-summary.md
          retention-days: 90
          
      - name: Update step summary
        run: |
          cat quality-gate-summary.md >> $GITHUB_STEP_SUMMARY
          
      - name: Comment on PR with quality gate results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea  # v7.0.1
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('quality-gate-summary.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
            
      - name: Set quality gate status
        run: |
          if [ "${{ steps.calculate-score.outputs.gate_result }}" = "FAIL" ]; then
            echo "Quality gate failed. Score: ${{ steps.calculate-score.outputs.overall_score }}% (required: ${{ steps.calculate-score.outputs.threshold }}%)"
            exit 1
          else
            echo "Quality gate passed. Score: ${{ steps.calculate-score.outputs.overall_score }}% (required: ${{ steps.calculate-score.outputs.threshold }}%)"
            exit 0
          fi
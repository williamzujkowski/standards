name: Quarterly System Review

on:
  schedule:
    # Run on January 1st, April 1st, July 1st, and October 1st at 1 AM UTC
    - cron: '0 1 1 1,4,7,10 *'
  workflow_dispatch:
    inputs:
      include_full_audit:
        description: 'Include full system audit'
        required: false
        default: true
        type: boolean
      generate_executive_summary:
        description: 'Generate executive summary'
        required: false
        default: true
        type: boolean

env:
  REVIEW_VERSION: "1.0"
  PYTHON_VERSION: "3.11"

jobs:
  system-architecture-review:
    name: System Architecture Review
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1
        with:
          fetch-depth: 0  # Full history for comprehensive analysis
          
      - name: Setup Python
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install analysis dependencies
        run: |
          pip install --upgrade pip
          pip install pandas matplotlib seaborn numpy scipy
          pip install networkx graphviz  # For architecture visualization
          pip install gitpython pyyaml requests beautifulsoup4
          pip install jinja2  # For report templating
          
      - name: Comprehensive architecture analysis
        id: architecture-analysis
        run: |
          echo "## ðŸ—ï¸ Quarterly System Architecture Review" > architecture-review.md
          echo "" >> architecture-review.md
          echo "**Review Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> architecture-review.md
          echo "**Quarter:** Q$(( ($(date +%-m) - 1) / 3 + 1 )) $(date +%Y)" >> architecture-review.md
          echo "**Review Version:** ${{ env.REVIEW_VERSION }}" >> architecture-review.md
          echo "" >> architecture-review.md
          
          # Create comprehensive architecture analysis script
          cat > analyze_architecture.py << 'EOF'
          import os
          import json
          import subprocess
          from pathlib import Path
          from collections import defaultdict, Counter
          import networkx as nx
          import matplotlib.pyplot as plt
          import pandas as pd
          from datetime import datetime, timedelta
          import git
          import yaml
          
          def analyze_repository_structure():
              """Analyze the overall repository structure"""
              structure = {
                  'directories': {},
                  'file_types': Counter(),
                  'size_distribution': {},
                  'depth_analysis': {}
              }
              
              # Walk through repository structure
              for root, dirs, files in os.walk('.'):
                  # Skip certain directories
                  dirs[:] = [d for d in dirs if d not in ['.git', 'node_modules', 'dist', 'target', 'build']]
                  
                  depth = root.count(os.sep)
                  if depth not in structure['depth_analysis']:
                      structure['depth_analysis'][depth] = {'dirs': 0, 'files': 0}
                  
                  structure['depth_analysis'][depth]['dirs'] += len(dirs)
                  structure['depth_analysis'][depth]['files'] += len(files)
                  
                  for file in files:
                      file_path = os.path.join(root, file)
                      file_ext = Path(file).suffix.lower()
                      structure['file_types'][file_ext] += 1
                      
                      try:
                          size = os.path.getsize(file_path)
                          size_category = 'small' if size < 10000 else 'medium' if size < 100000 else 'large'
                          if size_category not in structure['size_distribution']:
                              structure['size_distribution'][size_category] = 0
                          structure['size_distribution'][size_category] += 1
                      except:
                          continue
              
              return structure
          
          def analyze_standards_ecosystem():
              """Analyze the standards ecosystem and relationships"""
              standards_analysis = {
                  'standards_files': [],
                  'cross_references': defaultdict(list),
                  'coverage_map': {},
                  'orphaned_standards': [],
                  'heavily_referenced': []
              }
              
              # Find all standards files
              standards_patterns = ['*_STANDARDS.md', 'UNIFIED_STANDARDS.md']
              standards_files = []
              
              for pattern in standards_patterns:
                  standards_files.extend(Path('.').rglob(pattern))
              
              standards_files = [f for f in standards_files if not any(skip in str(f) for skip in 
                               ['node_modules', '.git', 'dist'])]
              
              # Analyze each standards file
              reference_graph = nx.DiGraph()
              
              for std_file in standards_files:
                  try:
                      with open(std_file, 'r', encoding='utf-8') as f:
                          content = f.read()
                      
                      file_info = {
                          'path': str(std_file),
                          'size_kb': len(content) / 1024,
                          'word_count': len(content.split()),
                          'line_count': len(content.splitlines())
                      }
                      standards_analysis['standards_files'].append(file_info)
                      
                      # Add to graph
                      reference_graph.add_node(std_file.name)
                      
                      # Find references to other standards
                      for other_std in standards_files:
                          if other_std != std_file and other_std.name in content:
                              standards_analysis['cross_references'][std_file.name].append(other_std.name)
                              reference_graph.add_edge(std_file.name, other_std.name)
                  
                  except Exception as e:
                      continue
              
              # Analyze graph properties
              if reference_graph.nodes():
                  # Find orphaned standards (no incoming or outgoing references)
                  for node in reference_graph.nodes():
                      if reference_graph.in_degree(node) == 0 and reference_graph.out_degree(node) == 0:
                          standards_analysis['orphaned_standards'].append(node)
                  
                  # Find heavily referenced standards
                  in_degrees = dict(reference_graph.in_degree())
                  heavily_ref = sorted(in_degrees.items(), key=lambda x: x[1], reverse=True)[:5]
                  standards_analysis['heavily_referenced'] = heavily_ref
              
              return standards_analysis, reference_graph
          
          def analyze_validation_ecosystem():
              """Analyze the validation and testing ecosystem"""
              validation_analysis = {
                  'validation_scripts': [],
                  'test_coverage': {},
                  'workflow_analysis': {},
                  'automation_score': 0
              }
              
              # Find validation scripts
              validation_paths = ['scripts', 'tests']
              for path in validation_paths:
                  if Path(path).exists():
                      for script_file in Path(path).rglob('*.py'):
                          if any(keyword in script_file.name.lower() for keyword in 
                                ['validate', 'test', 'check', 'verify']):
                              try:
                                  with open(script_file, 'r', encoding='utf-8') as f:
                                      content = f.read()
                                  
                                  script_info = {
                                      'path': str(script_file),
                                      'size_kb': len(content) / 1024,
                                      'line_count': len(content.splitlines()),
                                      'functions': content.count('def '),
                                      'classes': content.count('class ')
                                  }
                                  validation_analysis['validation_scripts'].append(script_info)
                              except:
                                  continue
              
              # Analyze GitHub workflows
              workflow_dir = Path('.github/workflows')
              if workflow_dir.exists():
                  workflow_files = list(workflow_dir.glob('*.yml')) + list(workflow_dir.glob('*.yaml'))
                  
                  validation_analysis['workflow_analysis'] = {
                      'total_workflows': len(workflow_files),
                      'validation_workflows': 0,
                      'scheduled_workflows': 0,
                      'automated_workflows': 0
                  }
                  
                  for workflow_file in workflow_files:
                      try:
                          with open(workflow_file, 'r', encoding='utf-8') as f:
                              content = f.read()
                          
                          if any(keyword in content.lower() for keyword in 
                                ['validate', 'test', 'check', 'health']):
                              validation_analysis['workflow_analysis']['validation_workflows'] += 1
                          
                          if 'schedule:' in content:
                              validation_analysis['workflow_analysis']['scheduled_workflows'] += 1
                          
                          if any(trigger in content for trigger in ['push:', 'pull_request:']):
                              validation_analysis['workflow_analysis']['automated_workflows'] += 1
                      except:
                          continue
              
              # Calculate automation score
              scripts_score = min(50, len(validation_analysis['validation_scripts']) * 10)
              workflows_score = min(50, validation_analysis['workflow_analysis']['validation_workflows'] * 15)
              validation_analysis['automation_score'] = scripts_score + workflows_score
              
              return validation_analysis
          
          def analyze_evolution_patterns():
              """Analyze repository evolution patterns"""
              try:
                  repo = git.Repo('.')
                  
                  # Get commit history for last 3 months
                  since = datetime.now() - timedelta(days=90)
                  commits = list(repo.iter_commits(since=since))
                  
                  evolution_analysis = {
                      'commits_last_quarter': len(commits),
                      'active_contributors': len(set(commit.author.email for commit in commits)),
                      'file_change_patterns': {},
                      'commit_patterns': {},
                      'velocity_trends': []
                  }
                  
                  # Analyze commit patterns by month
                  monthly_commits = defaultdict(int)
                  for commit in commits:
                      month_key = commit.committed_datetime.strftime('%Y-%m')
                      monthly_commits[month_key] += 1
                  
                  evolution_analysis['commit_patterns'] = dict(monthly_commits)
                  
                  # Analyze file change patterns
                  file_changes = defaultdict(int)
                  for commit in commits[:100]:  # Limit to avoid performance issues
                      try:
                          for file_path in commit.stats.files:
                              if file_path.endswith('.md'):
                                  file_changes[file_path] += 1
                      except:
                          continue
                  
                  # Get top 10 most changed files
                  top_changed = sorted(file_changes.items(), key=lambda x: x[1], reverse=True)[:10]
                  evolution_analysis['file_change_patterns'] = dict(top_changed)
                  
                  return evolution_analysis
              except Exception as e:
                  return {'error': f'Git analysis failed: {str(e)}'}
          
          def generate_architecture_visualization(standards_graph):
              """Generate architecture visualization"""
              if not standards_graph.nodes():
                  return False
                  
              try:
                  plt.figure(figsize=(12, 8))
                  
                  # Use spring layout for better visualization
                  pos = nx.spring_layout(standards_graph, k=2, iterations=50)
                  
                  # Draw nodes
                  nx.draw_networkx_nodes(standards_graph, pos, 
                                       node_color='lightblue', 
                                       node_size=1000, 
                                       alpha=0.7)
                  
                  # Draw edges
                  nx.draw_networkx_edges(standards_graph, pos, 
                                       edge_color='gray', 
                                       arrows=True, 
                                       arrowsize=20, 
                                       alpha=0.6)
                  
                  # Draw labels
                  labels = {node: node.replace('_STANDARDS.md', '').replace('_', '\n') 
                           for node in standards_graph.nodes()}
                  nx.draw_networkx_labels(standards_graph, pos, labels, font_size=8)
                  
                  plt.title('Standards Cross-Reference Architecture', fontsize=14, fontweight='bold')
                  plt.axis('off')
                  plt.tight_layout()
                  plt.savefig('standards-architecture.png', dpi=300, bbox_inches='tight')
                  plt.close()
                  
                  return True
              except Exception as e:
                  print(f"Visualization failed: {e}")
                  return False
          
          # Run all analyses
          print("Analyzing repository structure...")
          repo_structure = analyze_repository_structure()
          
          print("Analyzing standards ecosystem...")
          standards_analysis, standards_graph = analyze_standards_ecosystem()
          
          print("Analyzing validation ecosystem...")
          validation_analysis = analyze_validation_ecosystem()
          
          print("Analyzing evolution patterns...")
          evolution_analysis = analyze_evolution_patterns()
          
          print("Generating architecture visualization...")
          viz_generated = generate_architecture_visualization(standards_graph)
          
          # Compile comprehensive analysis
          quarterly_analysis = {
              'analysis_date': datetime.now().isoformat(),
              'quarter': f"Q{((datetime.now().month - 1) // 3) + 1} {datetime.now().year}",
              'repository_structure': repo_structure,
              'standards_ecosystem': standards_analysis,
              'validation_ecosystem': validation_analysis,
              'evolution_patterns': evolution_analysis,
              'visualization_generated': viz_generated
          }
          
          # Save comprehensive analysis
          with open('quarterly-analysis.json', 'w') as f:
              json.dump(quarterly_analysis, f, indent=2, default=str)
          
          print("Quarterly analysis completed successfully")
          print(f"Standards files analyzed: {len(standards_analysis['standards_files'])}")
          print(f"Validation scripts found: {len(validation_analysis['validation_scripts'])}")
          print(f"Automation score: {validation_analysis['automation_score']}/100")
          print(f"Architecture visualization: {'Generated' if viz_generated else 'Failed'}")
          EOF
          
          python analyze_architecture.py > architecture-analysis.log 2>&1
          
          # Process results and create report sections
          if [ -f quarterly-analysis.json ]; then
            echo "### ðŸ“Š Architecture Overview" >> architecture-review.md
            echo "" >> architecture-review.md
            
            # Extract key metrics
            python -c "
import json
with open('quarterly-analysis.json') as f:
    analysis = json.load(f)

repo = analysis['repository_structure']
standards = analysis['standards_ecosystem']
validation = analysis['validation_ecosystem']

print('| Component | Count | Details |')
print('|-----------|-------|---------|')
print(f'| Standards Files | {len(standards[\"standards_files\"])} | Core documentation standards |')
print(f'| Validation Scripts | {len(validation[\"validation_scripts\"])} | Automated quality checks |')
print(f'| File Types | {len(repo[\"file_types\"])} | Different file formats |')
print(f'| Automation Score | {validation[\"automation_score\"]}/100 | Overall automation level |')
" >> architecture-review.md
            
            echo "" >> architecture-review.md
            echo "### ðŸ”— Standards Ecosystem Analysis" >> architecture-review.md
            echo "" >> architecture-review.md
            
            python -c "
import json
with open('quarterly-analysis.json') as f:
    analysis = json.load(f)

standards = analysis['standards_ecosystem']

if standards['heavily_referenced']:
    print('#### Most Referenced Standards')
    print()
    for std, refs in standards['heavily_referenced']:
        print(f'- **{std}**: {refs} incoming references')
    print()

if standards['orphaned_standards']:
    print('#### Orphaned Standards (No Cross-References)')
    print()
    for std in standards['orphaned_standards']:
        print(f'- {std}')
    print()
" >> architecture-review.md
            
            echo "### ðŸ”„ Evolution Patterns" >> architecture-review.md
            echo "" >> architecture-review.md
            
            python -c "
import json
with open('quarterly-analysis.json') as f:
    analysis = json.load(f)

evolution = analysis['evolution_patterns']

if 'error' not in evolution:
    print(f'- **Commits Last Quarter**: {evolution[\"commits_last_quarter\"]}')
    print(f'- **Active Contributors**: {evolution[\"active_contributors\"]}')
    print()
    
    if evolution['file_change_patterns']:
        print('#### Most Frequently Updated Files')
        print()
        for file_path, changes in list(evolution['file_change_patterns'].items())[:5]:
            print(f'- {file_path}: {changes} changes')
        print()
else:
    print('Evolution analysis encountered issues - check logs for details.')
    print()
" >> architecture-review.md
            
            # Extract metrics for outputs
            automation_score=$(python -c "import json; data=json.load(open('quarterly-analysis.json')); print(data['validation_ecosystem']['automation_score'])")
            standards_count=$(python -c "import json; data=json.load(open('quarterly-analysis.json')); print(len(data['standards_ecosystem']['standards_files']))")
            
            echo "automation_score=$automation_score" >> $GITHUB_OUTPUT
            echo "standards_count=$standards_count" >> $GITHUB_OUTPUT
          else
            echo "âŒ Architecture analysis failed - check logs for details" >> architecture-review.md
            echo "automation_score=0" >> $GITHUB_OUTPUT
            echo "standards_count=0" >> $GITHUB_OUTPUT
          fi
          
      - name: Upload architecture visualization
        if: steps.architecture-analysis.outputs.automation_score != '0'
        uses: actions/upload-artifact@c7d193f32edcb7bfad88892161225aeda64e9392  # v4.0.0
        with:
          name: architecture-visualization
          path: standards-architecture.png
          retention-days: 1095  # Keep for 3 years
          
      - name: Upload architecture analysis
        uses: actions/upload-artifact@c7d193f32edcb7bfad88892161225aeda64e9392  # v4.0.0
        with:
          name: quarterly-architecture-review
          path: architecture-review.md
          retention-days: 1095

  comprehensive-audit:
    name: Comprehensive System Audit
    runs-on: ubuntu-latest
    timeout-minutes: 60
    if: ${{ github.event.inputs.include_full_audit != 'false' }}
    
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1
        with:
          fetch-depth: 0
          
      - name: Setup Python
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install audit dependencies
        run: |
          pip install --upgrade pip
          pip install pandas matplotlib seaborn numpy
          pip install pyyaml requests beautifulsoup4 textstat
          pip install bandit safety  # Security auditing tools
          
      - name: Full system audit
        id: system-audit
        run: |
          echo "## ðŸ” Comprehensive System Audit" > system-audit.md
          echo "" >> system-audit.md
          
          # Create comprehensive audit script
          cat > comprehensive_audit.py << 'EOF'
          import os
          import subprocess
          import json
          from pathlib import Path
          import re
          from collections import defaultdict
          import yaml
          
          def audit_security():
              """Comprehensive security audit"""
              security_findings = {
                  'potential_secrets': [],
                  'file_permissions': [],
                  'configuration_security': [],
                  'script_security': []
              }
              
              # Check for potential secrets in files
              secret_patterns = [
                  r'password\s*=\s*["\'][^"\']+["\']',
                  r'api_key\s*=\s*["\'][^"\']+["\']',
                  r'secret\s*=\s*["\'][^"\']+["\']',
                  r'token\s*=\s*["\'][^"\']+["\']'
              ]
              
              for pattern in secret_patterns:
                  try:
                      result = subprocess.run(['grep', '-r', '-i', pattern, '.'], 
                                            capture_output=True, text=True)
                      if result.stdout:
                          matches = result.stdout.strip().split('\n')
                          for match in matches[:5]:  # Limit results
                              if not any(skip in match for skip in ['.git/', 'node_modules/', 'dist/']):
                                  security_findings['potential_secrets'].append(match)
                  except:
                      continue
              
              # Check file permissions
              for root, dirs, files in os.walk('.'):
                  dirs[:] = [d for d in dirs if d not in ['.git', 'node_modules', 'dist']]
                  
                  for file in files:
                      file_path = os.path.join(root, file)
                      try:
                          stat = os.stat(file_path)
                          mode = oct(stat.st_mode)[-3:]
                          if mode == '777' or mode == '666':
                              security_findings['file_permissions'].append(f"{file_path}: {mode}")
                      except:
                          continue
              
              return security_findings
          
          def audit_quality():
              """Comprehensive quality audit"""
              quality_findings = {
                  'markdown_quality': {},
                  'code_quality': {},
                  'documentation_completeness': {},
                  'consistency_issues': []
              }
              
              # Audit markdown files
              md_files = list(Path('.').rglob('*.md'))
              md_files = [f for f in md_files if not any(skip in str(f) for skip in 
                         ['node_modules', '.git', 'dist'])]
              
              total_words = 0
              readability_scores = []
              
              for md_file in md_files:
                  try:
                      with open(md_file, 'r', encoding='utf-8') as f:
                          content = f.read()
                      
                      word_count = len(content.split())
                      total_words += word_count
                      
                      # Check for common issues
                      issues = []
                      if len(content) < 100:
                          issues.append('Very short file')
                      if content.count('#') < 1:
                          issues.append('No headers found')
                      if not re.search(r'\[.*\]\(.*\)', content):
                          issues.append('No links found')
                      
                      if issues:
                          quality_findings['markdown_quality'][str(md_file)] = issues
                  except:
                      continue
              
              quality_findings['total_words'] = total_words
              quality_findings['files_analyzed'] = len(md_files)
              
              return quality_findings
          
          def audit_configuration():
              """Audit configuration files and settings"""
              config_findings = {
                  'yaml_files': [],
                  'json_files': [],
                  'configuration_issues': [],
                  'missing_configs': []
              }
              
              # Check YAML files
              yaml_files = list(Path('.').rglob('*.yaml')) + list(Path('.').rglob('*.yml'))
              yaml_files = [f for f in yaml_files if not any(skip in str(f) for skip in 
                           ['node_modules', '.git', 'dist'])]
              
              for yaml_file in yaml_files:
                  try:
                      with open(yaml_file, 'r', encoding='utf-8') as f:
                          yaml.safe_load(f)
                      config_findings['yaml_files'].append(str(yaml_file))
                  except Exception as e:
                      config_findings['configuration_issues'].append(f"{yaml_file}: {str(e)}")
              
              # Check JSON files
              json_files = list(Path('.').rglob('*.json'))
              json_files = [f for f in json_files if not any(skip in str(f) for skip in 
                           ['node_modules', '.git', 'dist'])]
              
              for json_file in json_files:
                  try:
                      with open(json_file, 'r', encoding='utf-8') as f:
                          json.load(f)
                      config_findings['json_files'].append(str(json_file))
                  except Exception as e:
                      config_findings['configuration_issues'].append(f"{json_file}: {str(e)}")
              
              # Check for expected configuration files
              expected_configs = [
                  'config/standards-schema.yaml',
                  'config/standards-api.json',
                  'config/MANIFEST.yaml'
              ]
              
              for config in expected_configs:
                  if not Path(config).exists():
                      config_findings['missing_configs'].append(config)
              
              return config_findings
          
          def audit_automation():
              """Audit automation and workflow coverage"""
              automation_findings = {
                  'workflow_coverage': {},
                  'script_analysis': {},
                  'automation_gaps': [],
                  'recommendations': []
              }
              
              # Analyze GitHub workflows
              workflow_dir = Path('.github/workflows')
              if workflow_dir.exists():
                  workflows = list(workflow_dir.glob('*.yml')) + list(workflow_dir.glob('*.yaml'))
                  
                  workflow_types = {
                      'validation': 0,
                      'testing': 0,
                      'deployment': 0,
                      'monitoring': 0,
                      'scheduled': 0
                  }
                  
                  for workflow in workflows:
                      try:
                          with open(workflow, 'r', encoding='utf-8') as f:
                              content = f.read().lower()
                          
                          if any(keyword in content for keyword in ['validate', 'check', 'verify']):
                              workflow_types['validation'] += 1
                          if any(keyword in content for keyword in ['test', 'pytest', 'jest']):
                              workflow_types['testing'] += 1
                          if any(keyword in content for keyword in ['deploy', 'release', 'publish']):
                              workflow_types['deployment'] += 1
                          if any(keyword in content for keyword in ['monitor', 'health', 'alert']):
                              workflow_types['monitoring'] += 1
                          if 'schedule:' in content:
                              workflow_types['scheduled'] += 1
                      except:
                          continue
                  
                  automation_findings['workflow_coverage'] = workflow_types
              
              # Analyze scripts
              script_dirs = ['scripts', 'tests']
              script_count = 0
              for script_dir in script_dirs:
                  if Path(script_dir).exists():
                      scripts = list(Path(script_dir).glob('*.py')) + list(Path(script_dir).glob('*.sh'))
                      script_count += len(scripts)
              
              automation_findings['script_analysis']['total_scripts'] = script_count
              
              # Identify automation gaps
              if workflow_types.get('validation', 0) < 2:
                  automation_findings['automation_gaps'].append('Limited validation workflows')
              if workflow_types.get('scheduled', 0) < 1:
                  automation_findings['automation_gaps'].append('No scheduled workflows')
              if script_count < 5:
                  automation_findings['automation_gaps'].append('Limited automation scripts')
              
              return automation_findings
          
          # Run all audits
          print("Running security audit...")
          security_audit = audit_security()
          
          print("Running quality audit...")
          quality_audit = audit_quality()
          
          print("Running configuration audit...")
          config_audit = audit_configuration()
          
          print("Running automation audit...")
          automation_audit = audit_automation()
          
          # Compile comprehensive audit results
          comprehensive_audit_results = {
              'audit_date': datetime.now().isoformat(),
              'security_audit': security_audit,
              'quality_audit': quality_audit,
              'configuration_audit': config_audit,
              'automation_audit': automation_audit
          }
          
          # Save audit results
          with open('comprehensive-audit.json', 'w') as f:
              json.dump(comprehensive_audit_results, f, indent=2, default=str)
          
          print("Comprehensive audit completed")
          print(f"Security issues found: {len(security_audit['potential_secrets'])}")
          print(f"Quality issues found: {len(quality_audit['markdown_quality'])}")
          print(f"Configuration issues: {len(config_audit['configuration_issues'])}")
          print(f"Automation gaps: {len(automation_audit['automation_gaps'])}")
          EOF
          
          python comprehensive_audit.py > audit-execution.log 2>&1
          
          # Process audit results into report
          if [ -f comprehensive-audit.json ]; then
            echo "### ðŸ”’ Security Audit Results" >> system-audit.md
            echo "" >> system-audit.md
            
            python -c "
import json
with open('comprehensive-audit.json') as f:
    audit = json.load(f)

security = audit['security_audit']
print(f'- **Potential Secrets Found**: {len(security[\"potential_secrets\"])}')
print(f'- **File Permission Issues**: {len(security[\"file_permissions\"])}')
print(f'- **Configuration Security**: {len(security[\"configuration_security\"])}')

if security['potential_secrets']:
    print()
    print('âš ï¸ **Security Issues Require Review**')
" >> system-audit.md
            
            echo "" >> system-audit.md
            echo "### ðŸ“Š Quality Audit Results" >> system-audit.md
            echo "" >> system-audit.md
            
            python -c "
import json
with open('comprehensive-audit.json') as f:
    audit = json.load(f)

quality = audit['quality_audit']
print(f'- **Files Analyzed**: {quality[\"files_analyzed\"]}')
print(f'- **Total Words**: {quality[\"total_words\"]:,}')
print(f'- **Quality Issues**: {len(quality[\"markdown_quality\"])}')
" >> system-audit.md
            
            echo "" >> system-audit.md
            echo "### âš™ï¸ Configuration Audit Results" >> system-audit.md
            echo "" >> system-audit.md
            
            python -c "
import json
with open('comprehensive-audit.json') as f:
    audit = json.load(f)

config = audit['configuration_audit']
print(f'- **YAML Files**: {len(config[\"yaml_files\"])}')
print(f'- **JSON Files**: {len(config[\"json_files\"])}')
print(f'- **Configuration Issues**: {len(config[\"configuration_issues\"])}')
print(f'- **Missing Configs**: {len(config[\"missing_configs\"])}')

if config['configuration_issues']:
    print()
    print('âš ï¸ **Configuration Issues Found**')
    for issue in config['configuration_issues'][:3]:
        print(f'- {issue}')
" >> system-audit.md
            
            echo "" >> system-audit.md
            echo "### ðŸ¤– Automation Audit Results" >> system-audit.md
            echo "" >> system-audit.md
            
            python -c "
import json
with open('comprehensive-audit.json') as f:
    audit = json.load(f)

automation = audit['automation_audit']
workflows = automation['workflow_coverage']

print('#### Workflow Coverage')
print()
print('| Type | Count |')
print('|------|-------|')
for workflow_type, count in workflows.items():
    print(f'| {workflow_type.title()} | {count} |')
print()

if automation['automation_gaps']:
    print('#### Automation Gaps')
    print()
    for gap in automation['automation_gaps']:
        print(f'- âš ï¸ {gap}')
    print()
" >> system-audit.md
            
            # Extract key metrics
            security_issues=$(python -c "import json; data=json.load(open('comprehensive-audit.json')); print(len(data['security_audit']['potential_secrets']))")
            quality_issues=$(python -c "import json; data=json.load(open('comprehensive-audit.json')); print(len(data['quality_audit']['markdown_quality']))")
            
            echo "security_issues=$security_issues" >> $GITHUB_OUTPUT
            echo "quality_issues=$quality_issues" >> $GITHUB_OUTPUT
          else
            echo "âŒ Comprehensive audit failed - check logs for details" >> system-audit.md
            echo "security_issues=999" >> $GITHUB_OUTPUT
            echo "quality_issues=999" >> $GITHUB_OUTPUT
          fi
          
      - name: Upload audit results
        uses: actions/upload-artifact@c7d193f32edcb7bfad88892161225aeda64e9392  # v4.0.0
        with:
          name: comprehensive-system-audit
          path: system-audit.md
          retention-days: 1095

  executive-summary:
    name: Executive Summary Generation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [system-architecture-review, comprehensive-audit]
    if: always() && github.event.inputs.generate_executive_summary != 'false'
    
    steps:
      - name: Download all reports
        uses: actions/download-artifact@c7d193f32edcb7bfad88892161225aeda64e9392  # v4.0.0
        with:
          path: quarterly-reports
          
      - name: Generate executive summary
        id: executive-summary
        run: |
          quarter="Q$(( ($(date +%-m) - 1) / 3 + 1 )) $(date +%Y)"
          
          echo "# ðŸ“Š Quarterly System Review - Executive Summary" > executive-summary.md
          echo "" >> executive-summary.md
          echo "**Quarter:** $quarter" >> executive-summary.md
          echo "**Review Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> executive-summary.md
          echo "**Review Type:** Comprehensive System Evaluation" >> executive-summary.md
          echo "" >> executive-summary.md
          
          echo "## ðŸŽ¯ Executive Overview" >> executive-summary.md
          echo "" >> executive-summary.md
          
          # Architecture analysis summary
          if [ "${{ needs.system-architecture-review.result }}" = "success" ]; then
            automation_score="${{ needs.system-architecture-review.outputs.automation_score }}"
            standards_count="${{ needs.system-architecture-review.outputs.standards_count }}"
            
            echo "### ðŸ—ï¸ System Architecture Status" >> executive-summary.md
            echo "" >> executive-summary.md
            echo "- **Standards Documents:** $standards_count active standards" >> executive-summary.md
            echo "- **Automation Level:** $automation_score/100" >> executive-summary.md
            echo "- **Architecture Review:** âœ… Completed successfully" >> executive-summary.md
            echo "" >> executive-summary.md
            
            # Determine automation status
            if [ "$automation_score" -ge 80 ]; then
              automation_status="excellent"
              echo "ðŸŽ‰ **Automation Status:** Excellent - High level of automation achieved" >> executive-summary.md
            elif [ "$automation_score" -ge 60 ]; then
              automation_status="good"
              echo "âœ… **Automation Status:** Good - Solid automation foundation" >> executive-summary.md
            else
              automation_status="needs_improvement"
              echo "âš ï¸ **Automation Status:** Needs Improvement - Automation gaps identified" >> executive-summary.md
            fi
          else
            echo "### âŒ System Architecture Status" >> executive-summary.md
            echo "" >> executive-summary.md
            echo "Architecture review failed. Investigation required." >> executive-summary.md
            automation_status="failed"
          fi
          echo "" >> executive-summary.md
          
          # Audit summary
          if [ "${{ needs.comprehensive-audit.result }}" = "success" ]; then
            security_issues="${{ needs.comprehensive-audit.outputs.security_issues }}"
            quality_issues="${{ needs.comprehensive-audit.outputs.quality_issues }}"
            
            echo "### ðŸ” System Audit Status" >> executive-summary.md
            echo "" >> executive-summary.md
            echo "- **Security Issues:** $security_issues items requiring review" >> executive-summary.md
            echo "- **Quality Issues:** $quality_issues items identified" >> executive-summary.md
            echo "- **Audit Scope:** Comprehensive system evaluation" >> executive-summary.md
            echo "" >> executive-summary.md
            
            # Determine audit status
            total_issues=$((security_issues + quality_issues))
            if [ "$total_issues" -eq 0 ]; then
              audit_status="excellent"
              echo "ðŸŽ‰ **Audit Status:** Excellent - No critical issues found" >> executive-summary.md
            elif [ "$total_issues" -le 5 ]; then
              audit_status="good"
              echo "âœ… **Audit Status:** Good - Minor issues identified" >> executive-summary.md
            else
              audit_status="needs_attention"
              echo "âš ï¸ **Audit Status:** Needs Attention - Multiple issues require resolution" >> executive-summary.md
            fi
          else
            echo "### âŒ System Audit Status" >> executive-summary.md
            echo "" >> executive-summary.md
            echo "Comprehensive audit failed. Manual review required." >> executive-summary.md
            audit_status="failed"
          fi
          echo "" >> executive-summary.md
          
          echo "## ðŸ“‹ Key Findings & Recommendations" >> executive-summary.md
          echo "" >> executive-summary.md
          
          echo "### âœ… Strengths" >> executive-summary.md
          echo "" >> executive-summary.md
          echo "- Comprehensive validation workflows are operational" >> executive-summary.md
          echo "- Multi-level monitoring system (daily/weekly/monthly/quarterly)" >> executive-summary.md
          echo "- Automated issue detection and reporting" >> executive-summary.md
          echo "- Structured standards documentation approach" >> executive-summary.md
          echo "" >> executive-summary.md
          
          echo "### ðŸŽ¯ Strategic Recommendations" >> executive-summary.md
          echo "" >> executive-summary.md
          echo "#### Immediate Actions (Next 30 Days)" >> executive-summary.md
          
          if [ "$audit_status" = "needs_attention" ] || [ "$audit_status" = "failed" ]; then
            echo "1. **Address Critical Issues:** Review and resolve audit findings" >> executive-summary.md
          fi
          
          if [ "$automation_status" = "needs_improvement" ]; then
            echo "2. **Enhance Automation:** Improve automation coverage and reliability" >> executive-summary.md
          fi
          
          echo "3. **Review Detailed Reports:** Examine all quarterly analysis artifacts" >> executive-summary.md
          echo "4. **Update Documentation:** Ensure all findings are documented" >> executive-summary.md
          echo "" >> executive-summary.md
          
          echo "#### Medium-term Goals (Next Quarter)" >> executive-summary.md
          echo "1. **Process Optimization:** Streamline validation and review processes" >> executive-summary.md
          echo "2. **Capability Enhancement:** Expand monitoring and analysis capabilities" >> executive-summary.md
          echo "3. **Quality Improvements:** Implement recommendations from analysis" >> executive-summary.md
          echo "4. **Strategic Planning:** Plan for next quarter's improvements" >> executive-summary.md
          echo "" >> executive-summary.md
          
          echo "## ðŸ“Š Quarterly Scorecard" >> executive-summary.md
          echo "" >> executive-summary.md
          echo "| Area | Score | Status | Trend |" >> executive-summary.md
          echo "|------|-------|--------|-------|" >> executive-summary.md
          
          # Calculate scores
          if [ "$automation_status" = "excellent" ]; then
            automation_score_display="A"
          elif [ "$automation_status" = "good" ]; then
            automation_score_display="B"
          else
            automation_score_display="C"
          fi
          
          if [ "$audit_status" = "excellent" ]; then
            audit_score_display="A"
          elif [ "$audit_status" = "good" ]; then
            audit_score_display="B"
          else
            audit_score_display="C"
          fi
          
          echo "| System Architecture | $automation_score_display | $automation_status | â†—ï¸ |" >> executive-summary.md
          echo "| Quality & Security | $audit_score_display | $audit_status | â†—ï¸ |" >> executive-summary.md
          echo "| Automation Coverage | $automation_score_display | $automation_status | â†—ï¸ |" >> executive-summary.md
          echo "| Process Maturity | B | good | â†—ï¸ |" >> executive-summary.md
          echo "" >> executive-summary.md
          
          echo "## ðŸ”„ Next Review Cycle" >> executive-summary.md
          echo "" >> executive-summary.md
          echo "- **Next Quarterly Review:** $(date -d '+3 months' +'%B %Y')" >> executive-summary.md
          echo "- **Monthly Deep Analysis:** $(date -d 'next month' +'%Y-%m-01')" >> executive-summary.md
          echo "- **Weekly Validations:** Continue automated schedule" >> executive-summary.md
          echo "- **Daily Health Checks:** Ongoing automated monitoring" >> executive-summary.md
          echo "" >> executive-summary.md
          
          echo "## ðŸ“š Supporting Documentation" >> executive-summary.md
          echo "" >> executive-summary.md
          echo "Detailed supporting documentation is available in the workflow artifacts:" >> executive-summary.md
          echo "" >> executive-summary.md
          echo "- **Architecture Review:** Complete system architecture analysis" >> executive-summary.md
          echo "- **Comprehensive Audit:** Detailed security, quality, and configuration audit" >> executive-summary.md
          echo "- **Architecture Visualization:** Standards cross-reference diagrams" >> executive-summary.md
          echo "" >> executive-summary.md
          
          echo "---" >> executive-summary.md
          echo "" >> executive-summary.md
          echo "*This executive summary was generated automatically by the Quarterly System Review workflow.*" >> executive-summary.md
          
          # Determine overall system status
          overall_status="healthy"
          if [ "$automation_status" = "failed" ] || [ "$audit_status" = "failed" ]; then
            overall_status="critical"
          elif [ "$automation_status" = "needs_improvement" ] || [ "$audit_status" = "needs_attention" ]; then
            overall_status="needs_attention"
          fi
          
          echo "overall_status=$overall_status" >> $GITHUB_OUTPUT
          
      - name: Upload executive summary
        uses: actions/upload-artifact@c7d193f32edcb7bfad88892161225aeda64e9392  # v4.0.0
        with:
          name: quarterly-executive-summary
          path: executive-summary.md
          retention-days: 2190  # Keep for 6 years
          
      - name: Update step summary
        run: |
          cat executive-summary.md >> $GITHUB_STEP_SUMMARY
          
      - name: Create strategic planning issue
        if: steps.executive-summary.outputs.overall_status != 'healthy'
        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea  # v7.0.1
        with:
          script: |
            const quarter = `Q${Math.floor((new Date().getMonth()) / 3) + 1} ${new Date().getFullYear()}`;
            const title = `ðŸ“Š Quarterly Strategic Planning Required - ${quarter}`;
            const body = `
            ## Quarterly Review Completed - Strategic Planning Required
            
            The quarterly system review has been completed and requires strategic planning attention.
            
            **Overall Status:** ${{ steps.executive-summary.outputs.overall_status }}
            **Quarter:** ${quarter}
            
            **Key Areas Requiring Attention:**
            - Architecture: ${{ needs.system-architecture-review.result }}
            - Security & Quality Audit: ${{ needs.comprehensive-audit.result }}
            - Automation Score: ${{ needs.system-architecture-review.outputs.automation_score }}/100
            
            **Run Details:**
            - Workflow: ${{ github.workflow }}
            - Run ID: ${{ github.run_id }}
            - Date: ${new Date().toUTCString()}
            
            ### Strategic Actions Required:
            1. Review the executive summary and detailed reports
            2. Develop strategic plan for next quarter improvements
            3. Assign ownership for critical findings
            4. Schedule review meetings with stakeholders
            5. Update strategic roadmap based on findings
            
            **Workflow Link:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            
            This issue was automatically created by the quarterly review workflow.
            `;
            
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['quarterly-review', 'strategic-planning', 'automated', 'executive']
            });